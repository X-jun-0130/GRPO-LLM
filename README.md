# GRPO-LLM


## 核心特点

- 支持远程奖励模型 + 并发打分
- 多种任务多种打分规则


## 流程

### 数据处理

使用 `data_preprocess.py` 将数据处理成模型训练所需格式

### 模型训练

- `reward_model_api.py`：远程部署生成式奖励模型调用
- `RewardManager.py`：支持并发打分（因 `math_verify` 库不支持并发，已对数学任务做区分处理）
- `rewards.py`：具体的奖励规则，针对不同任务设置不同奖励方案
- `count-statistics.py`：对保存日志中的数据进行分析

### 模型日志

- `tensorboard_log`：主要功能为读取最新的 tensorboard 记录，将其保存为 json 格式并进行可视化绘图

### 训练运行

执行以下命令进行模型训练：

```bash
sh model_grpo.sh
```


## 经验总结

### 微调与冷启动

- 部分垂直领域任务具有特殊性，通用模型可能不具备相应能力。对于这类任务，建议先利用通用模型进行微调或冷启动，使其掌握基本能力后再进行强化学习训练。
- 微调时，采样数据的答案风格最好与原基础模型保持一致。若风格差异过大，可能会损害原始模型的能力。
- 为避免降低模型在其他通用任务上的表现，补充少量高质量的通用训练集（如数学、代码等领域数据）是有益的。

### 拒绝采样

- 当微调或冷启动后，模型已掌握新任务的部分能力时，需进行拒绝采样。
- 具体操作：对于同一样本，使用新模型生成若干条回复，过滤掉全部回答正确的样本；同时，对于模型始终无法回答正确的数据集，也应进行过滤。
